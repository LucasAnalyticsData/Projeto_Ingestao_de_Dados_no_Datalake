{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4b2095-fdb4-4fcf-bbae-4f0ba8f31467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ğŸš€ Projeto: ConstruÃ§Ã£o da Tabela Gold - Fato Vendas\n",
    "\n",
    "## ğŸ“Œ VisÃ£o Geral\n",
    "\n",
    "Este mÃ³dulo representa a construÃ§Ã£o da **tabela de fato `fato_vendas`** na camada **Gold** do projeto de Data Lakehouse. A tabela Ã© derivada da camada Silver e consolidada com foco em **eficiÃªncia analÃ­tica**, **integridade dos dados**, **otimizaÃ§Ã£o de performance** e **auditoria completa**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Objetivo\n",
    "\n",
    "Criar uma tabela de fatos robusta que consolide informaÃ§Ãµes de vendas, garantindo:\n",
    "\n",
    "- ğŸ§¾ Unicidade por combinaÃ§Ã£o de `OrderID` + `ItemID`\n",
    "- ğŸ’° Registro de mÃ©tricas de vendas como `TotalAmount`, `Quantity` e `Price`\n",
    "- ğŸ•’ Auditoria temporal com `Created_at` e versionamento Delta\n",
    "- ğŸ”„ AtualizaÃ§Ãµes seguras com **MERGE Delta**\n",
    "- âš¡ Performance otimizada com **Z-ORDER** e **particionamento eficiente**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Detalhes TÃ©cnicos\n",
    "\n",
    "- **Fonte**: Camada Silver (`tabela_prata_desnormalizadas`)\n",
    "- **Destino**: Camada Gold (`gold.fato_vendas`)\n",
    "- **Particionamento**: `Status` (categoria do pedido)\n",
    "- **Chave de NegÃ³cio**: ComposiÃ§Ã£o de `OrderID` e `ItemID`\n",
    "- **Colunas da Tabela Fato Vendas**:\n",
    "  - `OrderID` (identificador do pedido)\n",
    "  - `ItemID` (identificador do item)\n",
    "  - `CustomerID` (identificador do cliente)\n",
    "  - `OrderDate` (data do pedido)\n",
    "  - `Status` (status do pedido)\n",
    "  - `Quantity` (quantidade)\n",
    "  - `Price` (preÃ§o unitÃ¡rio)\n",
    "  - `TotalAmount` (valor total)\n",
    "  - `Created_at` (timestamp de inserÃ§Ã£o na camada Gold)\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Etapas do Pipeline\n",
    "\n",
    "1. **CriaÃ§Ã£o do Banco de Dados `gold`**, se nÃ£o existir.\n",
    "2. **Leitura da Tabela Silver** contendo dados desnormalizados.\n",
    "3. **Cache do DataFrame** para otimizar mÃºltiplas operaÃ§Ãµes sequenciais.\n",
    "4. **CriaÃ§Ã£o da Coluna `Created_at`** para auditoria e rastreabilidade.\n",
    "5. **GeraÃ§Ã£o da Coluna `hash_value`** para identificaÃ§Ã£o de registros Ãºnicos.\n",
    "6. **RemoÃ§Ã£o de Duplicatas** usando o hash como referÃªncia de unicidade.\n",
    "7. **Uso de Window Function** para manter apenas o registro mais recente de cada combinaÃ§Ã£o `OrderID` + `ItemID`.\n",
    "8. **Auditoria Inicial**: contagem de registros Ãºnicos preparados para a Gold.\n",
    "9. **AplicaÃ§Ã£o do MERGE Delta**: inserÃ§Ã£o e atualizaÃ§Ã£o eficiente baseada nas chaves de negÃ³cio.\n",
    "10. **Auditoria Final**: contagem pÃ³s-processamento.\n",
    "11. **OtimizaÃ§Ã£o com Z-ORDER** por `OrderID` para aceleraÃ§Ã£o de leitura.\n",
    "12. **Limpeza com VACUUM** para liberaÃ§Ã£o de espaÃ§o de arquivos antigos.\n",
    "13. **Registro no CatÃ¡logo Hive/Unity Catalog** para permitir consultas SQL e governanÃ§a.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… BenefÃ­cios TÃ©cnicos Aplicados\n",
    "\n",
    "| TÃ©cnica                      | Finalidade                                                                 |\n",
    "|-----------------------------|----------------------------------------------------------------------------|\n",
    "| `MERGE` Delta               | AtualizaÃ§Ãµes incrementais seguras e sem duplicidade                        |\n",
    "| `sha2(hash)`                | IdentificaÃ§Ã£o rÃ¡pida e leve de duplicatas completas                        |\n",
    "| `dropDuplicates` + `Window` | RemoÃ§Ã£o de linhas redundantes com base nas chaves e timestamp              |\n",
    "| `cache()`                   | OtimizaÃ§Ã£o de mÃºltiplas operaÃ§Ãµes no mesmo DataFrame                       |\n",
    "| `Z-ORDER`                   | Melhoria significativa no tempo de resposta para filtros por `OrderID`     |\n",
    "| `VACUUM`                    | ReduÃ§Ã£o de uso de armazenamento e remoÃ§Ã£o de arquivos obsoletos            |\n",
    "| `current_timestamp()`       | Registro confiÃ¡vel de quando os dados foram inseridos                      |\n",
    "| Registro no catÃ¡logo        | Acesso via consultas SQL, dashboards BI e governanÃ§a centralizada          |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š MÃ©tricas UtilizÃ¡veis\n",
    "\n",
    "A tabela permite geraÃ§Ã£o de KPIs e anÃ¡lises como:\n",
    "\n",
    "- ğŸ›ï¸ Total de vendas por pedido ou cliente\n",
    "- ğŸ“ˆ EvoluÃ§Ã£o temporal de faturamento\n",
    "- ğŸ“¦ Itens mais vendidos\n",
    "- ğŸ§¾ Ticket mÃ©dio por cliente\n",
    "- ğŸ§­ Performance de vendas por status (`Status`) ou data (`OrderDate`)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§± Modelo Estrela (Star Schema)\n",
    "\n",
    "Esta tabela representa a **Fato Vendas** de um **modelo dimensional**, conectada a dimensÃµes como:\n",
    "\n",
    "- `dim_cliente` â†’ via `CustomerID`\n",
    "- `dim_tempo` â†’ via `OrderDate`\n",
    "- `dim_produto` (futura) â†’ via `ItemID`\n",
    "\n",
    "Permite anÃ¡lises OLAP em dashboards e modelos de Machine Learning supervisionado com base em comportamento de compra.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb0648d5-f810-4ef6-91ae-22902202a2c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting great_expectations\n  Obtaining dependency information for great_expectations from https://files.pythonhosted.org/packages/62/cd/71b371a7b219e583e96fb3dde787f785538a705a7678aae0e7a592d43a87/great_expectations-1.4.1-py3-none-any.whl.metadata\n  Downloading great_expectations-1.4.1-py3-none-any.whl.metadata (8.8 kB)\nCollecting altair<5.0.0,>=4.2.1 (from great_expectations)\n  Obtaining dependency information for altair<5.0.0,>=4.2.1 from https://files.pythonhosted.org/packages/18/62/47452306e84d4d2e67f9c559380aeb230f5e6ca84fafb428dd36b96a99ba/altair-4.2.2-py3-none-any.whl.metadata\n  Downloading altair-4.2.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: cryptography>=3.2 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (41.0.3)\nCollecting jinja2>=3 (from great_expectations)\n  Obtaining dependency information for jinja2>=3 from https://files.pythonhosted.org/packages/62/a1/3d680cbfd5f4b8f15abc1d571870c5fc3e594bb582bc3b64ea099db13e56/jinja2-3.1.6-py3-none-any.whl.metadata\n  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\nCollecting jsonschema>=2.5.1 (from great_expectations)\n  Obtaining dependency information for jsonschema>=2.5.1 from https://files.pythonhosted.org/packages/69/4a/4f9dbeb84e8850557c02365a0eee0649abe5eb1d84af92a25731c6c0f922/jsonschema-4.23.0-py3-none-any.whl.metadata\n  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\nCollecting marshmallow<4.0.0,>=3.7.1 (from great_expectations)\n  Obtaining dependency information for marshmallow<4.0.0,>=3.7.1 from https://files.pythonhosted.org/packages/34/75/51952c7b2d3873b44a0028b1bd26a25078c18f92f256608e8d1dc61b39fd/marshmallow-3.26.1-py3-none-any.whl.metadata\n  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting mistune>=0.8.4 (from great_expectations)\n  Obtaining dependency information for mistune>=0.8.4 from https://files.pythonhosted.org/packages/01/4d/23c4e4f09da849e127e9f123241946c23c1e30f45a88366879e064211815/mistune-3.1.3-py3-none-any.whl.metadata\n  Downloading mistune-3.1.3-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (23.2)\nCollecting posthog<4,>3 (from great_expectations)\n  Obtaining dependency information for posthog<4,>3 from https://files.pythonhosted.org/packages/54/e2/c158366e621562ef224f132e75c1d1c1fce6b078a19f7d8060451a12d4b9/posthog-3.25.0-py2.py3-none-any.whl.metadata\n  Downloading posthog-3.25.0-py2.py3-none-any.whl.metadata (3.0 kB)\nCollecting pydantic>=1.10.7 (from great_expectations)\n  Obtaining dependency information for pydantic>=1.10.7 from https://files.pythonhosted.org/packages/b0/1d/407b29780a289868ed696d1616f4aad49d6388e5a77f567dcd2629dcd7b8/pydantic-2.11.3-py3-none-any.whl.metadata\n  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n\u001B[?25l     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/65.2 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m65.2/65.2 kB\u001B[0m \u001B[31m3.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: pyparsing>=2.4 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (2.8.2)\nRequirement already satisfied: requests>=2.20 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (2.31.0)\nCollecting ruamel.yaml>=0.16 (from great_expectations)\n  Obtaining dependency information for ruamel.yaml>=0.16 from https://files.pythonhosted.org/packages/c2/36/dfc1ebc0081e6d39924a2cc53654497f967a084a436bb64402dfce4254d9/ruamel.yaml-0.18.10-py3-none-any.whl.metadata\n  Downloading ruamel.yaml-0.18.10-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: scipy>=1.6.0 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (1.11.1)\nCollecting tqdm>=4.59.0 (from great_expectations)\n  Obtaining dependency information for tqdm>=4.59.0 from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n\u001B[?25l     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/57.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m57.7/57.7 kB\u001B[0m \u001B[31m6.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: typing-extensions>=4.1.0 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (4.10.0)\nCollecting tzlocal>=1.2 (from great_expectations)\n  Obtaining dependency information for tzlocal>=1.2 from https://files.pythonhosted.org/packages/c2/14/e2a54fabd4f08cd7af1c07030603c3356b74da07f7cc056e600436edfa17/tzlocal-5.3.1-py3-none-any.whl.metadata\n  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: numpy>=1.22.4 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (1.23.5)\nRequirement already satisfied: pandas<2.2,>=1.3.0 in /databricks/python3/lib/python3.11/site-packages (from great_expectations) (1.5.3)\nRequirement already satisfied: entrypoints in /databricks/python3/lib/python3.11/site-packages (from altair<5.0.0,>=4.2.1->great_expectations) (0.4)\nCollecting toolz (from altair<5.0.0,>=4.2.1->great_expectations)\n  Obtaining dependency information for toolz from https://files.pythonhosted.org/packages/03/98/eb27cc78ad3af8e302c9d8ff4977f5026676e130d28dd7578132a457170c/toolz-1.0.0-py3-none-any.whl.metadata\n  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.11/site-packages (from cryptography>=3.2->great_expectations) (1.15.1)\nCollecting MarkupSafe>=2.0 (from jinja2>=3->great_expectations)\n  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/f1/a4/aefb044a2cd8d7334c8a47d3fb2c9f328ac48cb349468cc31c20b539305f/MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting attrs>=22.2.0 (from jsonschema>=2.5.1->great_expectations)\n  Obtaining dependency information for attrs>=22.2.0 from https://files.pythonhosted.org/packages/77/06/bb80f5f86020c4551da315d78b3ab75e8228f89f0162f2c3a819e407941a/attrs-25.3.0-py3-none-any.whl.metadata\n  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\nCollecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.5.1->great_expectations)\n  Obtaining dependency information for jsonschema-specifications>=2023.03.6 from https://files.pythonhosted.org/packages/d1/0f/8910b19ac0670a0f80ce1008e5e751c4a57e14d2c4c13a482aa6079fa9d6/jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata\n  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting referencing>=0.28.4 (from jsonschema>=2.5.1->great_expectations)\n  Obtaining dependency information for referencing>=0.28.4 from https://files.pythonhosted.org/packages/c1/b1/3baf80dc6d2b7bc27a95a67752d0208e410351e3feb4eb78de5f77454d8d/referencing-0.36.2-py3-none-any.whl.metadata\n  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\nCollecting rpds-py>=0.7.1 (from jsonschema>=2.5.1->great_expectations)\n  Obtaining dependency information for rpds-py>=0.7.1 from https://files.pythonhosted.org/packages/e7/0c/91cf17dffa9a38835869797a9f041056091ebba6a53963d3641207e3d467/rpds_py-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading rpds_py-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas<2.2,>=1.3.0->great_expectations) (2022.7)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from posthog<4,>3->great_expectations) (1.16.0)\nCollecting monotonic>=1.5 (from posthog<4,>3->great_expectations)\n  Obtaining dependency information for monotonic>=1.5 from https://files.pythonhosted.org/packages/9a/67/7e8406a29b6c45be7af7740456f7f37025f0506ae2e05fb9009a53946860/monotonic-1.6-py2.py3-none-any.whl.metadata\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting backoff>=1.10.0 (from posthog<4,>3->great_expectations)\n  Obtaining dependency information for backoff>=1.10.0 from https://files.pythonhosted.org/packages/df/73/b6e24bd22e6720ca8ee9a85a0c4a2971af8497d8f3193fa05390cbd46e09/backoff-2.2.1-py3-none-any.whl.metadata\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: distro>=1.5.0 in /usr/lib/python3/dist-packages (from posthog<4,>3->great_expectations) (1.7.0)\nCollecting annotated-types>=0.6.0 (from pydantic>=1.10.7->great_expectations)\n  Obtaining dependency information for annotated-types>=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata\n  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\nCollecting pydantic-core==2.33.1 (from pydantic>=1.10.7->great_expectations)\n  Obtaining dependency information for pydantic-core==2.33.1 from https://files.pythonhosted.org/packages/0b/60/516484135173aa9e5861d7a0663dce82e4746d2e7f803627d8c25dfa5578/pydantic_core-2.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pydantic_core-2.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting typing-extensions>=4.1.0 (from great_expectations)\n  Obtaining dependency information for typing-extensions>=4.1.0 from https://files.pythonhosted.org/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl.metadata\n  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\nCollecting typing-inspection>=0.4.0 (from pydantic>=1.10.7->great_expectations)\n  Obtaining dependency information for typing-inspection>=0.4.0 from https://files.pythonhosted.org/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl.metadata\n  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.20->great_expectations) (2023.7.22)\nCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.16->great_expectations)\n  Obtaining dependency information for ruamel.yaml.clib>=0.2.7 from https://files.pythonhosted.org/packages/68/6e/264c50ce2a31473a9fdbf4fa66ca9b2b17c7455b31ef585462343818bd6c/ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=3.2->great_expectations) (2.21)\nDownloading great_expectations-1.4.1-py3-none-any.whl (5.0 MB)\n\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/5.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91mâ”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[91mâ•¸\u001B[0m\u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m1.6/5.0 MB\u001B[0m \u001B[31m50.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[91mâ•¸\u001B[0m \u001B[32m5.0/5.0 MB\u001B[0m \u001B[31m95.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m5.0/5.0 MB\u001B[0m \u001B[31m68.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading altair-4.2.2-py3-none-any.whl (813 kB)\n\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/813.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m813.6/813.6 kB\u001B[0m \u001B[31m81.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/134.9 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m134.9/134.9 kB\u001B[0m \u001B[31m23.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/88.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m88.5/88.5 kB\u001B[0m \u001B[31m9.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/50.9 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m50.9/50.9 kB\u001B[0m \u001B[31m6.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mistune-3.1.3-py3-none-any.whl (53 kB)\n\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/53.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m53.4/53.4 kB\u001B[0m \u001B[31m9.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading posthog-3.25.0-py2.py3-none-any.whl (89 kB)\n\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/89.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m89.1/89.1 kB\u001B[0m \u001B[31m16.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/443.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m443.6/443.6 kB\u001B[0m \u001B[31m62.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pydantic_core-2.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/2.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m111.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading ruamel.yaml-0.18.10-py3-none-any.whl (117 kB)\n\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/117.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m117.7/117.7 kB\u001B[0m \u001B[31m10.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/78.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m78.5/78.5 kB\u001B[0m \u001B[31m7.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/45.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m45.8/45.8 kB\u001B[0m \u001B[31m5.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\nDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nDownloading attrs-25.3.0-py3-none-any.whl (63 kB)\n\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/63.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m63.8/63.8 kB\u001B[0m \u001B[31m10.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\nDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\nDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading referencing-0.36.2-py3-none-any.whl (26 kB)\nDownloading rpds_py-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (389 kB)\n\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/389.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m389.4/389.4 kB\u001B[0m \u001B[31m33.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/739.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m739.1/739.1 kB\u001B[0m \u001B[31m71.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\nDownloading toolz-1.0.0-py3-none-any.whl (56 kB)\n\u001B[?25l   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m0.0/56.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m56.4/56.4 kB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: monotonic, tzlocal, typing-extensions, tqdm, toolz, ruamel.yaml.clib, rpds-py, mistune, marshmallow, MarkupSafe, backoff, attrs, annotated-types, typing-inspection, ruamel.yaml, referencing, pydantic-core, posthog, jinja2, pydantic, jsonschema-specifications, jsonschema, altair, great_expectations\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.10.0\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8ad0ce93-719c-4a88-91cc-6337bde2673e\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.6\n    Not uninstalling pydantic at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8ad0ce93-719c-4a88-91cc-6337bde2673e\n    Can't uninstall 'pydantic'. No files were found to uninstall.\nSuccessfully installed MarkupSafe-3.0.2 altair-4.2.2 annotated-types-0.7.0 attrs-25.3.0 backoff-2.2.1 great_expectations-1.4.1 jinja2-3.1.6 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 marshmallow-3.26.1 mistune-3.1.3 monotonic-1.6 posthog-3.25.0 pydantic-2.11.3 pydantic-core-2.33.1 referencing-0.36.2 rpds-py-0.24.0 ruamel.yaml-0.18.10 ruamel.yaml.clib-0.2.12 toolz-1.0.0 tqdm-4.67.1 typing-extensions-4.13.2 typing-inspection-0.4.0 tzlocal-5.3.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nğŸ” Registros Ãºnicos prontos para escrita na Gold: 29945\nâœ… Nova tabela Gold Fato escrita com dados Ãºnicos.\nğŸš€ Tabela otimizada com sucesso.\nğŸ“š Tabela Gold Fato registrada no catÃ¡logo.\nğŸš€ Z-ORDER aplicado com sucesso.\nğŸ§¹ EspaÃ§o de armazenamento liberado com VACUUM.\n\nğŸ“Œ Resumo final:\n- Total de registros Ãºnicos inseridos na Gold: 29945\n"
     ]
    }
   ],
   "source": [
    "!pip install great_expectations\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, current_timestamp, sha2, concat_ws\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "# 1. DefiniÃ§Ã£o dos Caminhos das Tabelas Delta\n",
    "# ------------------------------------------\n",
    "# Definimos os caminhos da camada Silver e Gold onde os dados sÃ£o lidos e gravados.\n",
    "# O caminho da camada Silver Ã© onde os dados brutos e desnormalizados estÃ£o armazenados.\n",
    "# O caminho da camada Gold serÃ¡ o destino da tabela Fato de Vendas tratada e otimizada.\n",
    "SILVER_PATH = \"abfss://silver@dlsprojetofixo.dfs.core.windows.net/tabela_prata_desnormalizadas\"\n",
    "GOLD_FACT_PATH = \"abfss://gold@dlsprojetofixo.dfs.core.windows.net/gold_fato_vendas\"\n",
    "GOLD_FACT_TABLE = \"gold.fato_vendas\"\n",
    "\n",
    "# ------------------------------------------\n",
    "# 2. CriaÃ§Ã£o do Banco de Dados Gold\n",
    "# ------------------------------------------\n",
    "# Criamos o banco de dados \"gold\" se ele nÃ£o existir, garantindo que as tabelas\n",
    "# sejam registradas dentro de um contexto de banco estruturado e organizado.\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS gold\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3. Leitura dos Dados da Camada Silver\n",
    "# ------------------------------------------\n",
    "# Carregamos os dados da camada Silver (dados desnormalizados) em um DataFrame.\n",
    "# O Delta Lake permite a leitura eficiente de dados armazenados em formato Delta.\n",
    "silver_df = spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4. AdiÃ§Ã£o de Timestamp de CriaÃ§Ã£o\n",
    "# ------------------------------------------\n",
    "# Adicionamos uma coluna \"Created_at\" com o timestamp atual, para auditar a data e hora\n",
    "# da inserÃ§Ã£o dos dados na camada Gold. Isso ajuda a garantir rastreabilidade.\n",
    "silver_df = silver_df.withColumn(\"Created_at\", current_timestamp())\n",
    "\n",
    "# ------------------------------------------\n",
    "# 5. Habilita a atualizaÃ§Ã£o automÃ¡tica de schema no Delta Lake\n",
    "# ------------------------------------------\n",
    "# Permite que o Delta aceite novos campos durante a escrita sem erro de schema incompatÃ­vel.\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "# ------------------------------------------\n",
    "#6. GeraÃ§Ã£o do Hash para IdentificaÃ§Ã£o de Duplicatas\n",
    "# ------------------------------------------\n",
    "# Criamos uma coluna de hash para cada linha do DataFrame. Essa coluna combina vÃ¡rias\n",
    "# colunas relevantes da tabela e gera uma \"assinatura\" Ãºnica para cada linha.\n",
    "# A vantagem de usar um hash Ã© que facilita a verificaÃ§Ã£o de duplicatas e a comparaÃ§Ã£o\n",
    "# entre os dados sem precisar de comparaÃ§Ãµes de vÃ¡rias colunas.\n",
    "silver_df = silver_df.withColumn(\n",
    "    \"hash_value\",\n",
    "    sha2(concat_ws(\"||\",\n",
    "        col(\"OrderID\").cast(\"string\"),\n",
    "        col(\"CustomerID\").cast(\"string\"),\n",
    "        col(\"ItemID\").cast(\"string\"),\n",
    "        col(\"OrderDate\").cast(\"string\"),\n",
    "        col(\"TotalAmount\").cast(\"string\"),\n",
    "        col(\"Quantity\").cast(\"string\"),\n",
    "        col(\"Price\").cast(\"string\"),\n",
    "        col(\"Status\").cast(\"string\")\n",
    "    ), 256)  # Gera um cÃ³digo hash de 256 bits dessa string concatenada\n",
    ")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 7. Limpeza de Dados Duplicados com Base no Hash\n",
    "# ------------------------------------------\n",
    "# Removemos as duplicatas, considerando o hash gerado como referÃªncia.\n",
    "# Isso garante que apenas uma instÃ¢ncia Ãºnica de cada linha seja mantida, melhorando a qualidade dos dados.\n",
    "# A coluna \"hash_value\" Ã© removida apÃ³s o processo de limpeza, pois nÃ£o Ã© necessÃ¡ria para a tabela final.\n",
    "fact_df_duplicado = (\n",
    "    silver_df\n",
    "    .dropDuplicates([\"hash_value\"])  # Remove linhas duplicadas baseadas no hash\n",
    "    .drop(\"hash_value\")  # Remove a coluna de hash apÃ³s a limpeza\n",
    "    .select(  \n",
    "        \"OrderDate\", \"OrderID\", \"Status\", \"Price\", \"Quantity\", \n",
    "        \"TotalAmount\", \"CustomerID\", \"ItemID\", \"Created_at\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 8. Contagem de Registros Ãšnicos\n",
    "# ------------------------------------------\n",
    "# Verificamos quantos registros Ãºnicos serÃ£o processados e inseridos na tabela Gold.\n",
    "# Isso ajuda a monitorar a eficiÃªncia do processo de deduplicaÃ§Ã£o e entender o volume de dados.\n",
    "unique_count = fact_df_duplicado.select(\"OrderID\", \"ItemID\").distinct().count()\n",
    "print(f\"ğŸ” Registros Ãºnicos prontos para escrita na Gold: {unique_count}\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 9. Cache do DataFrame para Melhorar a Performance\n",
    "# ------------------------------------------\n",
    "# O cache do DataFrame ajuda a evitar a recomputaÃ§Ã£o durante a execuÃ§Ã£o de operaÃ§Ãµes subsequentes.\n",
    "# Isso melhora a performance especialmente quando o DataFrame serÃ¡ utilizado vÃ¡rias vezes em operaÃ§Ãµes subsequentes.\n",
    "fact_df_duplicado.cache()\n",
    "\n",
    "# ------------------------------------------\n",
    "# 10. Escrita na Tabela Delta Gold\n",
    "# ------------------------------------------\n",
    "# Escrevemos os dados tratados e deduplicados na tabela Delta Gold. O modo \"append\" Ã© utilizado para adicionar os dados.\n",
    "# A opÃ§Ã£o \"mergeSchema\" permite que o esquema seja atualizado se necessÃ¡rio.\n",
    "fact_df_duplicado.write.format(\"delta\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .option(\"mergeSchema\", \"true\")\\\n",
    "    .save(GOLD_FACT_PATH)\n",
    "\n",
    "print(\"âœ… Nova tabela Gold Fato escrita com dados Ãºnicos.\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 11. OtimizaÃ§Ã£o da Tabela Delta Gold\n",
    "# ------------------------------------------\n",
    "# Realizamos a otimizaÃ§Ã£o da tabela Delta Gold com o comando \"OPTIMIZE\". Isso organiza fisicamente os arquivos\n",
    "# para melhorar a performance de leitura, especialmente para consultas que envolvem filtros.\n",
    "# A otimizaÃ§Ã£o Ã© feita com base no particionamento e organizaÃ§Ã£o dos arquivos no disco.\n",
    "spark.sql(f\"OPTIMIZE delta.`{GOLD_FACT_PATH}`\")\n",
    "\n",
    "print(\"ğŸš€ Tabela otimizada com sucesso.\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 12. Registro no CatÃ¡logo Hive/Unity Catalog\n",
    "# ------------------------------------------\n",
    "# Registramos a tabela Gold no catÃ¡logo para que ela esteja disponÃ­vel para consultas SQL e outros processos.\n",
    "# Isso permite a governanÃ§a dos dados e facilita a gestÃ£o e o uso dos dados na plataforma.\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {GOLD_FACT_TABLE}\n",
    "USING DELTA\n",
    "LOCATION '{GOLD_FACT_PATH}'\n",
    "\"\"\")\n",
    "print(\"ğŸ“š Tabela Gold Fato registrada no catÃ¡logo.\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 13. AplicaÃ§Ã£o de Z-ORDER para Otimizar Consultas\n",
    "# ------------------------------------------\n",
    "# O Z-ORDER Ã© aplicado para otimizar consultas que envolvem filtros em colunas especÃ­ficas.\n",
    "# Essa tÃ©cnica melhora o tempo de resposta ao garantir que os dados relacionados sejam fisicamente armazenados\n",
    "# prÃ³ximos uns dos outros no armazenamento, reduzindo o custo de I/O.\n",
    "spark.sql(f\"OPTIMIZE delta.`{GOLD_FACT_PATH}` ZORDER BY (OrderID)\")\n",
    "\n",
    "print(\"ğŸš€ Z-ORDER aplicado com sucesso.\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 14. Limpeza com VACUUM\n",
    "# ------------------------------------------\n",
    "# O comando VACUUM Ã© usado para remover arquivos obsoletos e antigos, liberando espaÃ§o de armazenamento.\n",
    "# Ele elimina arquivos que nÃ£o sÃ£o mais necessÃ¡rios apÃ³s as operaÃ§Ãµes de atualizaÃ§Ã£o e exclusÃ£o.\n",
    "# Isso ajuda a otimizar o uso de armazenamento no Delta Lake.\n",
    "spark.sql(f\"VACUUM delta.`{GOLD_FACT_PATH}` RETAIN 168 HOURS\")\n",
    "\n",
    "print(\"ğŸ§¹ EspaÃ§o de armazenamento liberado com VACUUM.\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 15. Resumo Final\n",
    "# ------------------------------------------\n",
    "# Exibimos um resumo do processo, com a contagem final de registros inseridos na tabela Gold.\n",
    "# Isso ajuda na monitoraÃ§Ã£o e verificaÃ§Ã£o do sucesso do pipeline.\n",
    "print(\"\\nğŸ“Œ Resumo final:\")\n",
    "print(f\"- Total de registros Ãºnicos inseridos na Gold: {unique_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c71a7ac-f2b7-4461-be9b-374d00ee1049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Log da execuÃ§Ã£o do job 'gold_fato_vendas' registrado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š Monitoramento e GovernanÃ§a - Log de ExecuÃ§Ã£o do Pipeline Gold (Fato Vendas)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# ------------------------------------------\n",
    "# 1. InÃ­cio da contagem do tempo de execuÃ§Ã£o\n",
    "# ------------------------------------------\n",
    "start_time = time.time()\n",
    "\n",
    "# ------------------------------------------\n",
    "# 2. ParÃ¢metros do log\n",
    "# ------------------------------------------\n",
    "job_name = \"gold_fato_vendas\"\n",
    "status = \"SUCESSO\"\n",
    "erro = None\n",
    "\n",
    "try:\n",
    "    # Quantidade de registros Ãºnicos inseridos, jÃ¡ definida anteriormente no seu pipeline\n",
    "    qtd_linhas = unique_count\n",
    "\n",
    "except Exception as e:\n",
    "    status = \"ERRO\"\n",
    "    erro = str(e)\n",
    "    qtd_linhas = 0\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3. CÃ¡lculo do tempo total da execuÃ§Ã£o\n",
    "# ------------------------------------------\n",
    "tempo_total = round(time.time() - start_time, 2)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4. DefiniÃ§Ã£o do schema do log para evitar erro de inferÃªncia\n",
    "# ------------------------------------------\n",
    "schema_log = StructType([\n",
    "    StructField(\"job_name\", StringType(), True),\n",
    "    StructField(\"data_execucao\", StringType(), True),\n",
    "    StructField(\"qtd_linhas\", IntegerType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"erro\", StringType(), True),\n",
    "    StructField(\"tempo_total_segundos\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# ------------------------------------------\n",
    "# 5. CriaÃ§Ã£o do DataFrame de log\n",
    "# ------------------------------------------\n",
    "log_execucao_df = spark.createDataFrame([(\n",
    "    job_name,\n",
    "    datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    qtd_linhas,\n",
    "    status,\n",
    "    erro,\n",
    "    tempo_total\n",
    ")], schema=schema_log)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 6. Escrita do log no Delta Lake (pasta de log especÃ­fica da Fato Vendas)\n",
    "# ------------------------------------------\n",
    "log_execucao_df.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(\"abfss://gold@dlsprojetofixo.dfs.core.windows.net/log_execucoes_gold_fato_vendas\")\n",
    "\n",
    "print(f\"ğŸ“Œ Log da execuÃ§Ã£o do job '{job_name}' registrado com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42640471-b762-4127-a4db-7db12051dc5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.log_execucoes_gold_fato_vendas\n",
    "    USING DELTA\n",
    "    LOCATION 'abfss://gold@dlsprojetofixo.dfs.core.windows.net/log_execucoes_gold_fato_vendas'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cac11d5-b914-4c15-83a0-b30aea2d01f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>job_name</th><th>data_execucao</th><th>qtd_linhas</th><th>status</th><th>erro</th><th>tempo_total_segundos</th></tr></thead><tbody><tr><td>gold_fato_vendas</td><td>2025-04-22 02:48:15</td><td>29945</td><td>SUCESSO</td><td>null</td><td>0.0</td></tr><tr><td>gold_fato_vendas</td><td>2025-04-23 00:29:34</td><td>29945</td><td>SUCESSO</td><td>null</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "gold_fato_vendas",
         "2025-04-22 02:48:15",
         29945,
         "SUCESSO",
         null,
         0.0
        ],
        [
         "gold_fato_vendas",
         "2025-04-23 00:29:34",
         29945,
         "SUCESSO",
         null,
         0.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 4
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "job_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "data_execucao",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "qtd_linhas",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "erro",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tempo_total_segundos",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM gold.log_execucoes_gold_fato_vendas"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4348439708604004,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Camada_Gold_FatoVendas_Upgrade",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}