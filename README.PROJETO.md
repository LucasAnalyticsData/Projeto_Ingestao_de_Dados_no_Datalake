# ğŸ“ Projeto de Engenharia de Dados - Pipeline Bronze â†’ Silver â†’ Gold

### ğŸ‘¨â€ğŸ’¼ Autor: Lucas Sousa Santos Oliveira
Especialista em FinanÃ§as em transiÃ§Ã£o para Engenharia de Dados | PÃ³s-graduaÃ§Ã£o em Big Data e Cloud Computing

---

## ğŸ¯ Objetivo do Projeto

Construir um pipeline de dados robusto e escalÃ¡vel utilizando Databricks com Delta Lake, que implemente as boas prÃ¡ticas modernas de ingestÃ£o, tratamento, enriquecimento e entrega de dados em mÃºltiplas camadas (Bronze â†’ Silver â†’ Gold), garantindo performance, rastreabilidade e governanÃ§a de dados para consumo analÃ­tico.

---

## ğŸ§± Arquitetura em Camadas

```mermaid
graph TD
  A[Camada Bronze - Dados Brutos] --> B[Camada Silver - Dados Tratados]
  B --> C[Camada Gold - Modelo AnalÃ­tico]
  C --> D[Consumo: Power BI / Databricks SQL / API]
```

---

## ğŸ”½ Camada Bronze (IngestÃ£o e ExplosÃ£o de Dados)

### ğŸ› ï¸ TransformaÃ§Ãµes Aplicadas
- ğŸ” **Explode de arrays**: transforma mÃºltiplos itens por pedido em mÃºltiplas linhas (`explode(Items)`) para granularidade analÃ­tica.
- ğŸ” **ExtraÃ§Ã£o de nested fields**: acesso e projeÃ§Ã£o de colunas internas como `ShippingAddress.City`, `Item.Price`, etc.
- ğŸ·ï¸ **PadronizaÃ§Ã£o de colunas** com `alias()` para consistÃªncia semÃ¢ntica.
- ğŸ“… **Particionamento por data** (`OrderDate`) para acelerar filtros temporais.
- â›“ï¸ **Registro dinÃ¢mico no catÃ¡logo** com verificaÃ§Ã£o condicional e criaÃ§Ã£o automÃ¡tica da tabela Delta caso ainda nÃ£o exista.

### âš™ï¸ TÃ©cnicas AvanÃ§adas Utilizadas
| TÃ©cnica                      | BenefÃ­cio                                                                 |
|-----------------------------|---------------------------------------------------------------------------|
| ğŸ”„ `Auto Loader (cloudFiles)` | IngestÃ£o incremental com detecÃ§Ã£o automÃ¡tica de arquivos Delta            |
| ğŸ¯ `explode()`              | Permite granularidade por item, essencial para anÃ¡lises de vendas         |
| ğŸ§Š `Delta Lake`             | Suporte a transaÃ§Ãµes ACID, time travel e schema evolution                 |
| ğŸ§­ `checkpointLocation`     | Garante consistÃªncia e retomada segura em caso de falha                   |
| ğŸ“Š `ZORDER BY`              | Otimiza leitura por `OrderID` e `CustomerID`                              |
| ğŸ” `VerificaÃ§Ã£o condicional`| CriaÃ§Ã£o da tabela no catÃ¡logo apenas quando necessÃ¡rio                    |

### ğŸ§ª ValidaÃ§Ã£o Final
- âœ… Contagem de registros gravados.
- ğŸš€ OtimizaÃ§Ã£o com `OPTIMIZE ZORDER`.
- ğŸ“š Registro automÃ¡tico no catÃ¡logo Hive/Unity Catalog.

---

## ğŸª„ Camada Silver (Tratamento, Enriquecimento e Modelagem)

### ğŸ› ï¸ Principais TransformaÃ§Ãµes
- ğŸ”„ ConversÃ£o de tipos (`OrderDate` para `Date`) para facilitar anÃ¡lises temporais.
- âŒ RemoÃ§Ã£o de colunas ambÃ­guas e irrelevantes (`Date_Time_Load`).
- ğŸ§¹ Limpeza de duplicatas e valores nulos (`dropDuplicates()` + `na.drop()`).
- ğŸ”— **DesnormalizaÃ§Ã£o com joins**: enriquecimento entre vendas e clientes via `CustomerID`.
- ğŸ§¾ PadronizaÃ§Ã£o com ordenaÃ§Ã£o explÃ­cita de colunas.
- ğŸ•’ InclusÃ£o de campo de auditoria (`last_updated`) com `current_timestamp()`.
- ğŸ”„ Reparticionamento por `OrderDate` para performance de escrita e leitura.

### âš™ï¸ TÃ©cnicas AvanÃ§adas Utilizadas
| TÃ©cnica                     | BenefÃ­cio                                                               |
|----------------------------|-------------------------------------------------------------------------|
| ğŸ”„ `DeltaTable.merge()`     | Garantia de atualizaÃ§Ã£o incremental sem sobrescrita total               |
| ğŸ“¦ `broadcast join`         | OtimizaÃ§Ã£o de joins entre tabelas de tamanhos desbalanceados           |
| ğŸ§¹ `dropDuplicates()` + `na.drop()` | RemoÃ§Ã£o de dados inconsistentes ou incompletos               |
| ğŸ§Š `cache()`                | Evita recomputaÃ§Ã£o em mÃºltiplas transformaÃ§Ãµes                         |
| ğŸ§± `OPTIMIZE ZORDER`        | Organiza fisicamente os dados para leitura eficiente por colunas-chave  |
| ğŸ§¼ `VACUUM`                 | Libera espaÃ§o removendo arquivos obsoletos                              |
| ğŸ“š `Registro no metastore` | Permite acesso governado via SQL, notebooks e Power BI                 |

---

## ğŸ¥‡ Camada Gold (Modelo Dimensional e Fato de Vendas)

### ğŸ“ DimensÃ£o Cliente

#### ğŸ¯ Objetivo
- ğŸ™‹â€â™‚ï¸ Dados Ãºnicos e limpos por `CustomerID`.
- ğŸ•’ InclusÃ£o de campo de auditoria (`Created_at`).
- ğŸ”„ AtualizaÃ§Ãµes eficientes com `MERGE Delta`.
- ğŸš€ OtimizaÃ§Ã£o com `Z-ORDER` e particionamento por `Country`, `State`.

#### âš™ï¸ Etapas
1. CriaÃ§Ã£o do banco `gold` (se nÃ£o existir).
2. Leitura da Silver com dados desnormalizados.
3. Reparticionamento por `Country`, `State`.
4. Limpeza com `dropDuplicates()` + `na.drop()`.
5. InclusÃ£o do campo `Created_at`.
6. Cache para otimizaÃ§Ã£o de mÃºltiplas etapas.
7. Auditoria com contagem antes e depois do `MERGE`.
8. Escrita com `MERGE` baseado em `CustomerID`.
9. `OPTIMIZE ZORDER` e `VACUUM`.
10. Registro no catÃ¡logo.

#### âœ… TÃ©cnicas Aplicadas
- `MERGE Delta`, `partitionBy`, `cache`, `Z-ORDER`, `VACUUM`, `registro no metastore`

---

### ğŸ“ DimensÃ£o Produto

#### ğŸ¯ Objetivo
- ğŸ“¦ Dados Ãºnicos por `ItemID`
- ğŸ“ˆ Estrutura otimizada para filtros e anÃ¡lises de produto
- ğŸ”„ MERGE Delta + Created_at para rastreabilidade
- Particionamento por `ItemID` (100 partiÃ§Ãµes)

#### âš™ï¸ Etapas
1. Leitura da Silver com controle de partiÃ§Ã£o
2. SeleÃ§Ã£o de `ItemID`, `ProductName`
3. RemoÃ§Ã£o de duplicatas e nulos
4. InclusÃ£o do `Created_at`
5. `cache()` e contagem antes/depois
6. Escrita com `MERGE` baseado em `ItemID`
7. OtimizaÃ§Ã£o com `Z-ORDER(ItemID)`
8. `VACUUM` e registro no catÃ¡logo

#### âœ… TÃ©cnicas Aplicadas
- `MERGE`, `dropDuplicates`, `partitionBy`, `Z-ORDER`, `registro no catÃ¡logo`

---

### ğŸ“ Fato Vendas

#### ğŸ¯ Objetivo
- ğŸ§¾ Garantir unicidade por `OrderID` + `ItemID`
- ğŸ’° Consolidar mÃ©tricas: `TotalAmount`, `Price`, `Quantity`
- ğŸ•’ Rastreabilidade com `Created_at`
- ğŸ”„ AtualizaÃ§Ã£o via `MERGE` com controle de duplicidade

#### âš™ï¸ Etapas
1. CriaÃ§Ã£o do banco gold
2. Leitura da Silver (tabela desnormalizada)
3. GeraÃ§Ã£o de `Created_at`
4. GeraÃ§Ã£o de `hash_value` com `sha2()` para identificar duplicatas
5. `dropDuplicates(hash)` e `window()` para manter registros mais recentes
6. Auditoria (contagem antes/depois)
7. Escrita com `MERGE (OrderID + ItemID)`
8. `Z-ORDER(OrderID)`, `VACUUM` e registro no catÃ¡logo

#### âœ… TÃ©cnicas Aplicadas
- `MERGE`, `sha2(hash)`, `dropDuplicates`, `window`, `cache`, `Z-ORDER`, `VACUUM`, `registro no catÃ¡logo`

---

## ğŸ“ˆ Monitoramento e Qualidade

- ğŸ” **Expectations com Delta Live Tables** para validaÃ§Ã£o automÃ¡tica de regras de qualidade
- ğŸ”” **Alertas** com Jobs + notificaÃ§Ãµes
- ğŸ§¾ **Logs e mÃ©tricas de execuÃ§Ã£o** persistidos em Delta Lake

---

## ğŸ§  ConclusÃ£o

Este projeto demonstra a construÃ§Ã£o de uma arquitetura Lakehouse completa com separaÃ§Ã£o lÃ³gica e fÃ­sica de dados, automaÃ§Ã£o de ingestÃ£o, transformaÃ§Ã£o confiÃ¡vel, e preparaÃ§Ã£o otimizada para consumo analÃ­tico com governanÃ§a. As tÃ©cnicas aplicadas â€” como `MERGE`, `Z-ORDER`, `Auto Loader`, `partitionBy`, `broadcast join`, `cache`, `VACUUM`, e `checkpoint` â€” evidenciam domÃ­nio profundo da stack Spark/Delta Lake e boas prÃ¡ticas modernas de engenharia de dados em nuvem.

> "Engenharia de dados Ã© mais do que mover bits â€” Ã© sobre criar pontes entre dados brutos e decisÃµes inteligentes."
