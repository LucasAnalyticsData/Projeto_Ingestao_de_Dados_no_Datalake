# ğŸ§  Guia Explicativo: Boas PrÃ¡ticas na Camada Bronze

Este documento reÃºne as boas prÃ¡ticas aplicadas na **Camada Bronze** de um pipeline de dados moderno, com foco em **escalabilidade**, **performance**, **controle de schema** e **economia de recursos em nuvem**.

---

## ğŸ”· Camada Bronze (IngestÃ£o de Dados Brutos)

### âœ… FunÃ§Ã£o principal:
- **Captura** dados brutos de mÃºltiplas fontes (arquivos CSV, APIs, sistemas legados).
- **Preserva a fidelidade dos dados**, sem agregaÃ§Ãµes ou transformaÃ§Ãµes analÃ­ticas.
- Serve como **ponto de rastreabilidade e auditoria** dentro do Data Lakehouse.

---

## ğŸ› ï¸ TÃ©cnicas e Melhorias Implementadas:

| TÃ©cnica                                | Justificativa TÃ©cnica                                                                 |
|----------------------------------------|----------------------------------------------------------------------------------------|
| ğŸ” **Auto Loader com `trigger(once)`** | IngestÃ£o incremental, com leitura eficiente e econÃ´mica de arquivos novos             |
| ğŸ§¹ **`dropDuplicates()` + `na.drop()`**| Elimina duplicidades e registros nulos, melhorando a qualidade desde a origem         |
| ğŸ“… **Particionamento por data**        | Organiza os dados no disco para melhorar performance de leitura                       |
| ğŸ§¬ **Delta Lake com schema evolution** | Permite evoluÃ§Ã£o segura do schema com histÃ³rico e tolerÃ¢ncia a mudanÃ§as estruturais   |
| ğŸ“Œ **Checkpointing**                   | Garante recuperaÃ§Ã£o e consistÃªncia mesmo em falhas ou reprocessamentos                |

---

## 1ï¸âƒ£ Leitura Eficiente com Auto Loader

### âŒ Antes: (forma tradicional)
```python
spark.read.format("csv").load("caminho/dados")
```

ğŸ” **Problemas desta abordagem:**
- Releitura de todos os arquivos existentes a cada execuÃ§Ã£o.
- Baixa escalabilidade com crescimento do volume de dados.
- AusÃªncia de gerenciamento de metadados ou versionamento.

---

### âœ… SoluÃ§Ã£o Moderna: Structured Streaming + Auto Loader

```python
df_bronze = (
    spark.readStream
        .format("cloudFiles")  # Auto Loader da Databricks otimizado para arquivos em nuvem
        .option("cloudFiles.format", "csv")  # Define o formato de entrada
        .option("cloudFiles.schemaLocation", schema_path)  # Caminho para armazenar e versionar o schema
        .option("cloudFiles.inferColumnTypes", "true")  # Detecta automaticamente tipos como Integer, Date, etc.
        .load(source_path)  # Caminho onde os arquivos sÃ£o depositados (armazenamento externo/nuvem)
)
```

### ğŸ” ExplicaÃ§Ã£o tÃ©cnica linha por linha:
- `readStream`: ativa o modo de leitura contÃ­nua (Structured Streaming).
- `"cloudFiles"`: usa Auto Loader, que monitora novas adiÃ§Ãµes no diretÃ³rio de forma eficiente.
- `"csv"`: permite lidar com arquivos legados e fontes comuns de ingestÃ£o inicial.
- `schemaLocation`: grava a estrutura inferida para manter consistÃªncia entre execuÃ§Ãµes.
- `inferColumnTypes`: evita que tudo seja lido como `String`, gerando schemas ricos.
- `load(source_path)`: inicia a leitura incremental baseada em novos arquivos.

---

## 2ï¸âƒ£ Escrita em Delta com Particionamento e Trigger Controlado

```python
df_final.writeStream     .format("delta")  # Escrita no formato Delta Lake, com ACID e versionamento nativo
    .partitionBy("data_carga")  # Cria subpastas por data, otimizando futuras consultas
    .outputMode("append")  # Adiciona novos dados sem sobrescrever os existentes
    .option("checkpointLocation", checkpoint_path)  # Armazena estado para recuperaÃ§Ã£o e controle de duplicaÃ§Ãµes
    .trigger(once=True)  # Executa como um batch controlado (ideal para orquestraÃ§Ãµes com tempo e custo fixos)
    .start(bronze_path)  # Caminho de saÃ­da no Data Lake (camada Bronze)
```

### ğŸ’¡ Justificativas:
- `format("delta")`: permite escrita com confiabilidade transacional (ACID) e suporte a `schema evolution`.
- `partitionBy("data_carga")`: reduz o custo de leitura e escrita com pushdown predicates e menor I/O.
- `outputMode("append")`: evita reprocessamentos e mantÃ©m o histÃ³rico dos dados ingeridos.
- `trigger(once=True)`: ideal para pipelines por lote controlado, com menor custo e sem necessidade de cluster ativo contÃ­nuo.
- `checkpointLocation`: essencial para tolerÃ¢ncia a falhas e controle de duplicidade nos dados ingeridos.

---

## 3ï¸âƒ£ Controle de Schema + Qualidade Inicial

```python
df_limpado = df.dropDuplicates().na.drop()
```

### âœ… Por que aplicar logo na Bronze?
- **`dropDuplicates()`**: evita registros repetidos que inflacionam os dados e distorcem mÃ©tricas futuras.
- **`na.drop()`**: remove registros com campos nulos, especialmente em chaves crÃ­ticas (como IDs e datas).
- Promove **qualidade mÃ­nima** mesmo em dados brutos, prevenindo propagaÃ§Ã£o de sujeira atÃ© Silver/Gold.

---

## âœ… Tabela Resumo

| EstratÃ©gia                           | Valor para o Pipeline                                                              |
|-------------------------------------|------------------------------------------------------------------------------------|
| Auto Loader (`cloudFiles`)          | Leitura incremental de novos arquivos com alta performance                        |
| Trigger Once                        | Economia de recursos e controle sobre execuÃ§Ãµes                                   |
| Delta Lake                          | SeguranÃ§a transacional, versionamento e auditoria                                 |
| PartitionBy (`data_carga`)          | OtimizaÃ§Ã£o de leitura, compressÃ£o e pushdown                                      |
| Checkpoint                          | ResiliÃªncia a falhas, controle de estado entre execuÃ§Ãµes                          |
| Schema Evolution (`mergeSchema`)    | TolerÃ¢ncia a alteraÃ§Ãµes sem quebras estruturais                                   |
| Qualidade (`dropDuplicates`, `na`)  | Previne retrabalho e sujeira nas prÃ³ximas camadas                                 |

---

## ğŸ§  ConclusÃ£o TÃ©cnica

A Camada Bronze, quando bem projetada, nÃ£o Ã© apenas um â€œrepositÃ³rio brutoâ€, mas uma **base sÃ³lida e rastreÃ¡vel** para todo o pipeline analÃ­tico. A aplicaÃ§Ã£o dessas boas prÃ¡ticas permite que vocÃª:

- **Ganhe performance desde a ingestÃ£o**, evitando gargalos futuros.
- **Reduza custo de cluster**, usando `trigger(once)` e partiÃ§Ãµes eficientes.
- **Aumente a confiabilidade** com Delta Lake e checkpoints.
- **Simplifique manutenÃ§Ã£o**, mesmo com evoluÃ§Ã£o de schema e crescimento de volume.

---

> ğŸ” *"NÃ£o existe dado bom na camada Silver sem uma Bronze bem feita." â€” Engenharia de Dados moderna*

