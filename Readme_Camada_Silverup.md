# ğŸª™ Camada Silver â€“ Enriquecimento e OtimizaÃ§Ã£o de Vendas

A **Camada Silver** Ã© responsÃ¡vel por transformar os dados brutos da Bronze em uma estrutura limpa, confiÃ¡vel e pronta para anÃ¡lise. Aqui, aplicamos tÃ©cnicas avanÃ§adas de engenharia de dados para gerar uma tabela desnormalizada de vendas (modelo **One Big Table**) com foco em performance e facilidade de consumo analÃ­tico.

> "Na Silver, os dados deixam de ser apenas rastreÃ¡veis e se tornam analiticamente poderosos."

---

## ğŸ“ LocalizaÃ§Ãµes
| Camada | Path |
|--------|------|
| Bronze | `abfss://bronze@dlsprojetofixo.dfs.core.windows.net` |
| Silver | `abfss://silver@dlsprojetofixo.dfs.core.windows.net` |

---

## ğŸ” Fontes Utilizadas
- `vendas_lucas_exploded`: InformaÃ§Ãµes detalhadas de vendas, item a item.
- `clientes_lucas`: Dados cadastrais dos clientes.

---

## âš™ï¸ Pipeline Detalhado e Comentado

### 1. âœ… Leitura das Tabelas Bronze com Delta Lake
```python
vendas = spark.read.format("delta").load("path_para_bronze/vendas_lucas_exploded")
clientes = spark.read.format("delta").load("path_para_bronze/clientes_lucas")
```
ğŸ“Œ *Uso do formato Delta garante performance, leitura otimizada e suporte a schema evolution.*

---

### 2. âœ… ConversÃ£o de tipos e limpeza inicial
```python
from pyspark.sql.functions import col, to_date

vendas = vendas.withColumn("OrderDate", to_date(col("OrderDate"), "yyyy-MM-dd"))
clientes = clientes.drop("Date_Time_Load")
```
ğŸ“Œ *ConversÃ£o de OrderDate melhora anÃ¡lises temporais. A coluna `Date_Time_Load` Ã© irrelevante para anÃ¡lise e foi removida.*

---

### 3. âœ… Evitar DuplicaÃ§Ã£o e limpeza de nulos
```python
vendas = vendas.dropDuplicates().na.drop()
clientes = clientes.dropDuplicates().na.drop()
```
ğŸ“Œ *Remove ruÃ­dos como duplicatas e linhas com dados ausentes, elevando a qualidade desde jÃ¡.*

---

### 4. âœ… Enriquecimento via Broadcast Join
```python
from pyspark.sql.functions import broadcast

silver = vendas.join(broadcast(clientes), on="CustomerID", how="inner")
```
ğŸ“Œ *Broadcast Join evita shuffle e acelera o processamento em joins assimÃ©tricos.*

---

### 5. âœ… InclusÃ£o de coluna de auditoria
```python
from pyspark.sql.functions import current_timestamp

silver = silver.withColumn("last_updated", current_timestamp())
```
ğŸ“Œ *Permite rastreabilidade da carga, essencial em ambientes produtivos e auditoria.*

---

### 6. âœ… OrdenaÃ§Ã£o das colunas para consistÃªncia
```python
colunas_ordenadas = [
    "OrderID", "CustomerID", "OrderDate", "TotalAmount", "Quantity", "Price",
    "CustomerName", "Country", "Email", "Phone", "last_updated"
]
silver = silver.select(colunas_ordenadas)
```
ğŸ“Œ *Padroniza a estrutura para facilitar leitura e uso por dashboards e analistas.*

---

### 7. âœ… Reparticionamento e otimizaÃ§Ã£o por OrderDate
```python
silver = silver.repartition("OrderDate")
silver.cache()
```
ğŸ“Œ *Reparticiona logicamente os dados, melhorando escrita e leitura. Uso de `cache()` evita reprocessamentos em etapas seguintes.*

---

### 8. âœ… Escrita incremental com merge (upsert)
```python
from delta.tables import DeltaTable

tabela_silver = "gold.fato_vendas"
tabela_path = "abfss://silver@dlsprojetofixo.dfs.core.windows.net/tabela_prata_desnormalizadas"

if DeltaTable.isDeltaTable(spark, tabela_path):
    delta_tabela = DeltaTable.forPath(spark, tabela_path)
    (
        delta_tabela.alias("target")
        .merge(
            silver.alias("source"),
            "target.OrderID = source.OrderID AND target.CustomerID = source.CustomerID"
        )
        .whenMatchedUpdateAll()
        .whenNotMatchedInsertAll()
        .execute()
    )
else:
    silver.write.format("delta").mode("overwrite").partitionBy("OrderDate").save(tabela_path)
```
ğŸ“Œ *Uso de `merge` permite ingestÃ£o incremental sem sobrescrever os dados, garantindo performance e consistÃªncia.*

---

### 9. âœ… Registro no Metastore para consumo via SQL
```python
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS {tabela_silver}
    USING DELTA
    LOCATION '{tabela_path}'
""")
```
ğŸ“Œ *Facilita uso via notebooks SQL, BI tools e democratiza o acesso aos dados confiÃ¡veis.*

---

### 10. âœ… OtimizaÃ§Ã£o e limpeza da tabela
```python
spark.sql(f"OPTIMIZE {tabela_silver} ZORDER BY (OrderDate, OrderID)")
spark.sql(f"VACUUM {tabela_silver} RETAIN 168 HOURS")
```
ğŸ“Œ *OPTIMIZE reorganiza fisicamente os arquivos para leitura por colunas filtradas. VACUUM limpa arquivos obsoletos, reduzindo custo de armazenamento.*

---

## âœ… TÃ©cnicas AvanÃ§adas Utilizadas
| TÃ©cnica | BenefÃ­cio |
|--------|-----------|
| DeltaTable.merge() | AtualizaÃ§Ã£o incremental robusta, sem sobrescrita total |
| broadcast join | Acelera joins entre grandes e pequenas tabelas |
| dropDuplicates() + na.drop() | Dados limpos desde a Silver, evitando falhas em anÃ¡lises |
| cache() | Reaproveitamento em etapas intensas sem reprocessamento |
| OPTIMIZE ZORDER | Performance superior em consultas analÃ­ticas |
| VACUUM | ReduÃ§Ã£o de custos e organizaÃ§Ã£o do Delta Lake |
| Metastore SQL | Permite acesso padronizado por notebooks e BI |

---

## ğŸ§  ConclusÃ£o
Essa arquitetura Silver mostra um pipeline robusto, escalÃ¡vel e alinhado com as melhores prÃ¡ticas de engenharia de dados no Databricks. Ao seguir essa abordagem, garantimos consistÃªncia, performance e rastreabilidade ponta-a-ponta para as anÃ¡lises futuras.

> "Silver transforma dados brutos em valor analÃ­tico com elegÃ¢ncia e eficiÃªncia."

---




